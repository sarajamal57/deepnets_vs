{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST run on local via clean scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, errno\n",
    "\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "import copy, joblib, shutil\n",
    "from collections import Counter\n",
    "\n",
    "module_path = os.path.dirname(os.getcwd())+'/../src'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "#SEED_tf=42  #=0\n",
    "#import tensorflow as tf\n",
    "#tf.compat.v1.set_random_seed(SEED_tf) \n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "from light_curve import LightCurve\n",
    "import functions_preprocess as m_preprocess\n",
    "import functions_keras as m_func\n",
    "#import nets_ae_clf as m_nets #m_dln\n",
    "\n",
    "SEED=0\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_run_last as m_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "E0615 19:19:37.322119 140734833571264 execution.py:657] File `'../src/setup_notebook.py'` not found.\n"
     ]
    }
   ],
   "source": [
    "%run ../src/setup_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_preprocess.print_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "        dict_nruns = {0 : 'classifier_MLP_meta',\n",
    "                      #\n",
    "                      1 : 'classifier_direct_RNN',\n",
    "                      2 : 'classifier_direct_tCNN',\n",
    "                      3 : 'classifier_direct_dTCN',\n",
    "                      #\n",
    "                      4 : 'autoencoder_RNN',\n",
    "                      5 : 'autoencoder_tCNN',\n",
    "                      6 : 'autoencoder_dTCN',\n",
    "                      #\n",
    "                      7 : 'composite_net_RNN',\n",
    "                      8 : 'composite_net_tCNN',\n",
    "                      9 : 'composite_net_dTCN',\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_store         = '/Users/sjamal/Downloads/local_disk_store/'\n",
    "data_store         = '/Users/sjamal/git/deepnet_vs/data/MACHO/'\n",
    "output_store       = '/Users/sjamal/git/deepnet_vs/outputs/trained_models/'\n",
    "\n",
    "data_id            = 'blue'\n",
    " \n",
    "nb_epoch           = 200 #200\n",
    "\n",
    "## ------------------------------------------------------------------------------------ ##\n",
    "## ------------------------------------------------------------------------------------ ##\n",
    "model_type         = 'tCNN'\n",
    "run_id             = 8\n",
    "## ------------------------------------------------------------------------------------ ##\n",
    "## ------------------------------------------------------------------------------------ ##\n",
    "\n",
    "\n",
    "sim_type           = m_runs.dict_nruns[run_id]\n",
    "\n",
    "sizenet            = 16\n",
    "num_layers         = 1\n",
    "drop_frac          = 0.25\n",
    "batch_size         = 128\n",
    "learning_rate      = 5e-4\n",
    "\n",
    "validation_split   = 0.20\n",
    "\n",
    "gpu_frac           = 0.00\n",
    "\n",
    "m_causal           = True\n",
    "m_categorical      = True\n",
    "\n",
    "m_embedding        = 8 if run_id in np.r_[m_runs.list_ae, m_runs.list_composite] else None\n",
    "\n",
    "\n",
    "m_raw              = False         ## normalized data versus initial values\n",
    "m_padding          = True#         ## fixed-length data versus initial data\n",
    "m_fold             = True          ## phase-folded verus time series\n",
    "\n",
    "m_meta             = True\n",
    "\n",
    "m_reductionfactor  = 2\n",
    "\n",
    "## dTCN\n",
    "if run_id in [3,6,9]:\n",
    "    n_stacks       = num_layers\n",
    "    kernel_wavenet = 1\n",
    "    max_dilation   = 2\n",
    "    kernel_size    = 3\n",
    "    m_activation   = 'wavenet'\n",
    "## tCNN\n",
    "if run_id in [2,5,8]:\n",
    "    max_dilation   = 2\n",
    "    kernel_size    = 5\n",
    "    m_activation   = 'tanh'\n",
    "## RNN (LSTM; GRU)\n",
    "if run_id in [1,4,7]:\n",
    "    bidirectional  = True\n",
    "\n",
    "\n",
    "arg_dict = {'data_id'           : data_id,\n",
    "            'run_id'            : run_id ,\n",
    "            #\n",
    "            'data_store'        : data_store ,\n",
    "            'output_store'      : output_store,\n",
    "            #\n",
    "            'sizenet'           : sizenet ,\n",
    "            'num_layers'        : num_layers, \n",
    "            'drop_frac'         : drop_frac ,\n",
    "            'batch_size'        : batch_size,\n",
    "            'nb_epoch'          : nb_epoch,\n",
    "            'model_type'        : model_type, \n",
    "            'learning_rate'     : learning_rate ,\n",
    "            #\n",
    "            'embedding'         : m_embedding,\n",
    "            'add_dense'         : True ,\n",
    "            'validation_split'  : validation_split ,\n",
    "            #\n",
    "            'categorical'       : m_categorical,\n",
    "            'causal'            : m_causal ,\n",
    "            'sim_type'          : sim_type+('_fixedlength' if m_padding else '_generator'),\n",
    "            'gpu_frac'          : gpu_frac, \n",
    "            #\n",
    "            'add_metadata'      : m_meta,\n",
    "            #\n",
    "            'loss_AE'           : 'mae',\n",
    "            'loss_CLF'          : 'categorical_crossentropy' if m_categorical else 'logcosh',\n",
    "            'metrics_CLF'       : 'categorical_accuracy'     if m_categorical else 'accuracy',\n",
    "            #\n",
    "            'use_raw'           : m_raw,        # normalized data versus initial values\n",
    "            'padding'           : m_padding ,   # fixed-length data versus initial data\n",
    "            'period_fold'       : m_fold ,      # phase-folded verus time series\n",
    "            #\n",
    "            'no_train'          : False\n",
    "           }\n",
    "\n",
    "## RNN (LSTM; GRU)\n",
    "if run_id in [1,4,7]:\n",
    "    arg_dict['bidirectional']     = bidirectional      # rnn\n",
    "## tCNN\n",
    "if run_id in [2,5,8]:\n",
    "    arg_dict['m_activation']      = m_activation\n",
    "    arg_dict['kernel_size']       = kernel_size\n",
    "    arg_dict['max_dilation']      = max_dilation\n",
    "## dTCN\n",
    "if run_id in [3,6,9]:    \n",
    "    arg_dict['m_reductionfactor'] = m_reductionfactor  # tcnn\n",
    "    arg_dict['m_activation']      = m_activation\n",
    "    arg_dict['kernel_size']       = kernel_size\n",
    "    arg_dict['kernel_wavenet']    = kernel_wavenet\n",
    "    arg_dict['n_stacks']          = n_stacks\n",
    "    arg_dict['max_dilation']      = max_dilation\n",
    "#\n",
    "\n",
    "\n",
    "args = m_func.parse_model_args(arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ############## 1 - Set args session ##############  ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importlib.reload(m_runs); importlib.reload(m_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_dict = m_runs.set_params_cline(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ############## 2 - Load stored data_structures  ############## ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importlib.reload(m_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lcs=None; input_metadata=None\n",
    "\n",
    "input_lcs, input_metadata, output_dict = m_runs.get_data(arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Metadata :', input_metadata['selected'].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ############## 3 - Train network(s) ############## ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importlib.reload(m_nets); \n",
    "importlib.reload(m_func); importlib.reload(m_runs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stime = time.time() \n",
    "\n",
    "if True:\n",
    "    m_runs.run_autoencoder(arg_dict, input_lcs, input_metadata, output_dict)\n",
    "    \n",
    "hours, rem = divmod(time.time() - stime, 3600) #timeit.default_timer()-stime\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"\\n*Execution time : {:0>2} h {:0>2} min {:05.2f} s\".format(int(hours), int(minutes), seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, errno\n",
    "\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "import copy, joblib, shutil\n",
    "from collections import Counter\n",
    "\n",
    "module_path = os.path.dirname(os.getcwd())+'/../src'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import functions_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
