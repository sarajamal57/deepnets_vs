{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of MACHO VS - example 1\n",
    "\n",
    "- Data-type     : MACHO VS (normalized, phase-folded, fixed-length light-curves)\n",
    "- Network type  : Composite network\n",
    "- Layer type    : tCNN\n",
    "- Configuration : (nFilters=16, nLayers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Note__\n",
    "-      The autoencoders and composite networks require fixed-length data to compile/fit the model. Limitation arises from the type of the bottleneck layers in the decoder modules.\n",
    "-      Direct classifiers can process fixed-length data or the initial time-series. The latter is processed through generator functions (fit_generator) to take into account the different length of each observed light-curves.\n",
    "***\n",
    "### DATA\n",
    "\n",
    "__Source__ : MACHO VS multiband photometry [red and blue bands] (Alcock et al., 1996)\n",
    "\n",
    "__Preprocessed data__ \n",
    "-      Fixed-length LCs   [raw/normalized], [phase-folded, time-series]\n",
    "-      Initial-length LCs [raw], [phase-folded/time-series]\n",
    "\n",
    "\n",
    "__Processing of multiband light-curves__ (Jamal and Bloom, 2020)\n",
    "-      '_Bband_'  &emsp;&nbsp; `data_id='blue'` \n",
    "-      '_merged_' &emsp;`data_id='rb'` \n",
    "-      '_hybrid_' &emsp;&nbsp; `data_id='multiple'`\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run ../setup_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import script_MACHO as m_runs\n",
    "\n",
    "SEED=0; np.random.seed(SEED)\n",
    "# SEED_tf=42; tf.compat.v1.set_random_seed(SEED_tf)   ## set in \"functions_keras.py\"****\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] Network hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "        dict_nruns = {0 : 'classifier_MLP_meta',\n",
    "                      #\n",
    "                      1 : 'classifier_direct_RNN',\n",
    "                      2 : 'classifier_direct_tCNN',\n",
    "                      3 : 'classifier_direct_dTCN',\n",
    "                      #\n",
    "                      4 : 'autoencoder_RNN',\n",
    "                      5 : 'autoencoder_tCNN',\n",
    "                      6 : 'autoencoder_dTCN',\n",
    "                      #\n",
    "                      7 : 'composite_net_RNN',\n",
    "                      8 : 'composite_net_tCNN',\n",
    "                      9 : 'composite_net_dTCN',\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------------------------------------------- ##\n",
    "## HYPERPARAMS CONFIGS\n",
    "## ---------------------------------------------- ##\n",
    "data_id            = 'blue'\n",
    "model_type         = 'tCNN'\n",
    "run_id             = 8             ## [0:9] in dict_nruns\n",
    "\n",
    "sizenet            = 16\n",
    "num_layers         = 1\n",
    "\n",
    "m_raw              = False         ## TRUE: normalized data,   FALSE: initial obs.\n",
    "m_padding          = True          ## TRUE: fixed-length data, FALSE: initial lengths\n",
    "m_fold             = True          ## TRUE: phase-folded LCs,  FALSE: time-series\n",
    "m_meta             = True          ## TRUE: metadata as ancillary input,  FALSE: none\n",
    "\n",
    "\n",
    "\n",
    "## ---------------------------------------------- ##\n",
    "## COMMON HYPERPARAMS\n",
    "## ---------------------------------------------- ##\n",
    "sim_type           = m_runs.dict_nruns[run_id]\n",
    "nb_epoch           = 200  \n",
    "drop_frac          = 0.25\n",
    "batch_size         = 128\n",
    "learning_rate      = 5e-4\n",
    "validation_split   = 0.20\n",
    "m_reductionfactor  = 2\n",
    "gpu_frac           = 0.00\n",
    "m_causal           = True\n",
    "m_categorical      = True\n",
    "\n",
    "## dTCN\n",
    "if run_id in [3,6,9]:\n",
    "    n_stacks       = num_layers\n",
    "    kernel_wavenet = 1\n",
    "    max_dilation   = 2\n",
    "    kernel_size    = 3\n",
    "    m_activation   = 'wavenet'\n",
    "## tCNN\n",
    "if run_id in [2,5,8]:\n",
    "    max_dilation   = 2\n",
    "    kernel_size    = 5\n",
    "    m_activation   = 'tanh'\n",
    "## RNN (LSTM; GRU)\n",
    "if run_id in [1,4,7]:\n",
    "    bidirectional  = True\n",
    "\n",
    "m_embedding        = 8 if run_id in np.r_[m_runs.list_ae, m_runs.list_composite] else None\n",
    "\n",
    "\n",
    "#data_store         = '/Users/sjamal/git/deepnets_vs/example_data/MACHO/'\n",
    "#output_store       = '/Users/sjamal/git/deepnets_vs/outputs/trained_models/'\n",
    "\n",
    "data_store         = '/../example_data/MACHO/'\n",
    "output_store       = '/../trained_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_dict = {'data_id'           : data_id,\n",
    "            'run_id'            : run_id ,\n",
    "            #\n",
    "            'data_store'        : data_store ,\n",
    "            'output_store'      : output_store,\n",
    "            #\n",
    "            'sizenet'           : sizenet ,\n",
    "            'num_layers'        : num_layers, \n",
    "            'drop_frac'         : drop_frac ,\n",
    "            'batch_size'        : batch_size,\n",
    "            'nb_epoch'          : nb_epoch,\n",
    "            'model_type'        : model_type, \n",
    "            'learning_rate'     : learning_rate ,\n",
    "            #\n",
    "            'embedding'         : m_embedding,\n",
    "            'add_dense'         : True ,\n",
    "            'validation_split'  : validation_split ,\n",
    "            #\n",
    "            'categorical'       : m_categorical,\n",
    "            'causal'            : m_causal ,\n",
    "            'sim_type'          : sim_type,\n",
    "            'gpu_frac'          : gpu_frac, \n",
    "            #\n",
    "            'loss_AE'           : 'mae',\n",
    "            'loss_CLF'          : 'categorical_crossentropy' if m_categorical else 'logcosh',\n",
    "            'metrics_CLF'       : 'categorical_accuracy'     if m_categorical else 'accuracy',\n",
    "            #\n",
    "            'use_raw'           : m_raw,        ## TRUE: normalized data,   FALSE: initial obs.\n",
    "            'padding'           : m_padding ,   ## TRUE: fixed-length data, FALSE: initial lengths\n",
    "            'period_fold'       : m_fold ,      ## TRUE: phase-folded LCs,  FALSE: time-series\n",
    "            'add_metadata'      : m_meta,       ## TRUE: metadata as ancillary input,  FALSE: none\n",
    "            #\n",
    "            'no_train'          : False\n",
    "           }\n",
    "\n",
    "## RNN (LSTM; GRU)\n",
    "if run_id in [1,4,7]:\n",
    "    arg_dict['bidirectional']     = bidirectional      \n",
    "## tCNN\n",
    "if run_id in [2,5,8]:\n",
    "    arg_dict['m_activation']      = m_activation       \n",
    "    arg_dict['kernel_size']       = kernel_size\n",
    "    arg_dict['max_dilation']      = max_dilation\n",
    "## dTCN\n",
    "if run_id in [3,6,9]:    \n",
    "    arg_dict['m_reductionfactor'] = m_reductionfactor  \n",
    "    arg_dict['m_activation']      = m_activation\n",
    "    arg_dict['kernel_size']       = kernel_size\n",
    "    arg_dict['kernel_wavenet']    = kernel_wavenet\n",
    "    arg_dict['n_stacks']          = n_stacks\n",
    "    arg_dict['max_dilation']      = max_dilation\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t# ------------------------------------------ #\n",
      "\t# --------[ SESSION - HYPERPARAMS ] -------- # \n",
      "\t# ------------------------------------------ #\n",
      "\n",
      "\t data_id \t: blue\n",
      "\t run_id \t: 8\n",
      "\t sim_type \t: composite_net_tCNN_fixedlength_06182020\n",
      "\t data_store \t: /Users/sjamal/git/deepnets_vs/notebooks/../example_data/MACHO/\n",
      "\t output_store \t: /Users/sjamal/git/deepnets_vs/notebooks/../trained_models/\n",
      "\t nb_passbands \t: 1\n",
      "\t sizenet \t: 16\n",
      "\t embedding \t: 8\n",
      "\t num_layers \t: 1\n",
      "\t drop_frac \t: 0.25\n",
      "\t batch_size \t: 128\n",
      "\t nb_epoch \t: 200\n",
      "\t model_type \t: tCNN\n",
      "\t learning_rate \t: 0.0005\n",
      "\t decode_type \t: None\n",
      "\t decode_layers \t: None\n",
      "\t bidirectional \t: False\n",
      "\t output_size_cw \t: None\n",
      "\t n_stacks \t: None\n",
      "\t max_dilation \t: 2\n",
      "\t m_reductionfactor \t: 2\n",
      "\t kernel_size \t: 5\n",
      "\t kernel_wavenet \t: 1\n",
      "\t m_activation \t: tanh\n",
      "\t do_featurizer \t: False\n",
      "\t config_wavenet \t: False\n",
      "\t use_skip_connections \t: False\n",
      "\t add_dense \t: True\n",
      "\t use_raw \t: False\n",
      "\t add_metadata \t: True\n",
      "\t causal \t: True\n",
      "\t aux_in \t: False\n",
      "\t categorical \t: True\n",
      "\t loss_weights_list \t: None\n",
      "\t validation_split \t: 0.2\n",
      "\t loss_AE \t: mae\n",
      "\t loss_CLF \t: categorical_crossentropy\n",
      "\t metrics_CLF \t: categorical_accuracy\n",
      "\t metrics_AE \t: None\n",
      "\t n_min \t: 200\n",
      "\t m_max \t: 20.0\n",
      "\t ss_resid \t: None\n",
      "\t lomb_score \t: None\n",
      "\t survey_files \t: None\n",
      "\t nbpoints \t: 200\n",
      "\t padding \t: True\n",
      "\t diff_time \t: False\n",
      "\t period_fold \t: True\n",
      "\t add_freqs \t: None\n",
      "\t gpu_frac \t: 0.0\n",
      "\t no_train \t: False\n",
      "\t patience \t: 20\n",
      "\t store_local \t: False\n",
      "\t pretrain \t: None\n",
      "\t finetune_rate \t: None\n",
      "\t pool \t: None\n",
      "\t sigma \t: 2e-09\n",
      "\t noisify \t: False\n",
      "\t metrics \t: {'clf_softmax_dense': 'categorical_accuracy', 'decode_pb0_time_dist': []}\n",
      "\t loss \t: {'clf_softmax_dense': 'categorical_crossentropy', 'decode_pb0_time_dist': 'mae'}\n",
      "\t loss_weights \t: {'clf_softmax_dense': 1.0, 'decode_pb0_time_dist': 1.0}\n"
     ]
    }
   ],
   "source": [
    "arg_dict = m_runs.set_params_cline(m_func.parse_model_args(arg_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] Load stored datastructures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sjamal/git/deepnets_vs/notebooks/../example_data/MACHO/preprocessed_data/pkl_fileformat/phasefold/fixed_lengths/Xnorm_fold_blue.pkl\n"
     ]
    }
   ],
   "source": [
    "input_lcs=None; input_metadata=None\n",
    "\n",
    "input_lcs, input_metadata, output_dict = m_runs.get_data(arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Metadata :', input_metadata['selected'].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] Train networks & store logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stime = time.time() \n",
    "\n",
    "if True:\n",
    "    m_runs.run_network(arg_dict, input_lcs, input_metadata, output_dict)\n",
    "    \n",
    "hours, rem = divmod(time.time() - stime, 3600) #timeit.default_timer()-stime\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"\\n*Execution time : {:0>2} h {:0>2} min {:05.2f} s\".format(int(hours), int(minutes), seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
